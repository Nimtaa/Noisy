{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Activation, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.signal\n",
    "import numpy as np\n",
    "import pandas\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "from matplotlib.pyplot import figure\n",
    "import random\n",
    "from heapq import nlargest\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(20, input_shape = (200,10), return_sequences = True))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='RMSprop',metrics=['binary_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "data = pandas.read_csv('./data/Learning/MyData/Turning/ut_another_classifier/data_8_9.csv')\n",
    "# data = pandas.read_csv('./data/Learning/MyData/Turning/ut_another_classifier/data_9_8.csv')\n",
    "\n",
    "\n",
    "X = data[['magneticHeading','gyroX','gyroY','gyroZ','Yaw','Roll','Pitch','accx','accy','accz']]\n",
    "\n",
    "target = data['TurnStatus']\n",
    "\n",
    "##### 5-fold corss validation\n",
    "\n",
    "##### fold 1 \n",
    "# X_train = X[:11410]\n",
    "# X_test = X[11410:20650]\n",
    "# y_train = target[:11410]\n",
    "# y_test = target[11410:20650]\n",
    "######\n",
    "\n",
    "##### fold 2\n",
    "# X_train= X[:20650]\n",
    "# X_test= X[20650:31950]\n",
    "# y_train= target[:20650]\n",
    "# y_test= target[20650:31950]\n",
    "######\n",
    "\n",
    "##### fold 3\n",
    "# X_train= X[:31950]\n",
    "# X_test= X[31950:40550]\n",
    "# y_train= target[:31950]\n",
    "# y_test= target[31950:40550]\n",
    "######\n",
    "\n",
    "##### fold 4\n",
    "# X_train = X[:40550]\n",
    "# X_test = X[40550:58237]\n",
    "# y_train = target[:40550]\n",
    "# y_test = target[40550:58237]\n",
    "######\n",
    "\n",
    "##### fold 5\n",
    "X_train = X[:58237]\n",
    "X_test = X[58237:65927]\n",
    "y_train = target[:58237]\n",
    "y_test = target[58237:65927]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X_train = X[:40000]\n",
    "# y_train = target[:40000]\n",
    "\n",
    "# X_test = X[40000:]\n",
    "# y_test = target[40000:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SmoothData(data):\n",
    "    size = data.shape[0] \n",
    "    dim = data.shape[1]\n",
    "    \n",
    "    smooth_data = np.zeros((size,dim))\n",
    "    \n",
    "    smooth_data[0] = data[0]\n",
    "    smooth_data[1] = data[1]\n",
    "    smooth_data[size-1] = data[size-1]\n",
    "    smooth_data[size-2] = data[size-2]\n",
    "    \n",
    "    for i in range(2, size-2):\n",
    "        smooth_data[i] = np.average([data[i], data[i-1], data[i+1]], axis=0)\n",
    "    return smooth_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train  = np.array(X_train)\n",
    "X_test  = np.array(X_test)\n",
    "y_train  = np.array(y_train)\n",
    "y_test  = np.array(y_test)\n",
    "\n",
    "X_train = SmoothData(X_train)\n",
    "X_test = SmoothData(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot  = []\n",
    "y_test_onehot = []\n",
    "for i in range(len(y_train)):\n",
    "    if(y_train[i]==0):\n",
    "        y_train_onehot.append(np.array([1,0]))\n",
    "    else:\n",
    "        y_train_onehot.append(np.array([0,1]))\n",
    "        \n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i]==0):\n",
    "        y_test_onehot.append(np.array([1,0]))\n",
    "    else:\n",
    "        y_test_onehot.append(np.array([0,1]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_onehot[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### shuffle the train data to decorelate the training window segment\n",
    "X_train_shuffle = []\n",
    "y_train_shuffle = []\n",
    "shuffle_index = random.sample(range(0, 57200), 572)\n",
    "for i in shuffle_index:\n",
    "    X_train_shuffle.append (np.array([X_train[i:i+200]]).reshape(200,10))\n",
    "    y_train_shuffle.append (np.array([y_train_onehot[i:i+200]]).reshape(200,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(np.array(X_train_shuffle),\n",
    "                    np.array(y_train_shuffle),\n",
    "                    epochs=200,\n",
    "                    validation_data = (np.array([X_train[i:i+200] for i in range(57200,58000,50)]),\n",
    "                                      np.array([y_train_onehot[i:i+200] for i in range(57200,58000,50)])),\n",
    "                    batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(np.array(X_test[:7600]).reshape(38,200,10))\n",
    "results = results.reshape(7600,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_class = []\n",
    "for i in results:\n",
    "#     if(max(i) < 0.5):\n",
    "#         result_class.append(0)\n",
    "#     else:\n",
    "    if(np.argmax(i)==0):\n",
    "        result_class.append(0)\n",
    "    else:\n",
    "        result_class.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utitlity Functions\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Computing the energy (on y-axis)\n",
    "\n",
    "First, scale the accelerometer signal to the 9.8 \n",
    "Second, iterate list of events, calculate energy of the y-axis \n",
    "\"\"\"\n",
    "\n",
    "def Energy(data):\n",
    "    data = [abs(number) for number in data]\n",
    "    return np.sum(np.power(data,2))\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "Scaling turn signal to m/s2\n",
    "\"\"\"\n",
    "def Scale(data):\n",
    "    scaled_data = data.copy()\n",
    "    for i in range(len(scaled_data)):\n",
    "        scaled_data[i] = scaled_data[i] * 9.8\n",
    "    return scaled_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating list of events and \n",
    "removing events with energy < 20 and duration lower than 30\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "ut = 0\n",
    "num_events = 0\n",
    "events = []\n",
    "freq = 100 \n",
    "scaled_data = Scale([X_test[i][8] for i in range(len(X_test))]) # ay\n",
    "\n",
    "for i in range(len(result_class)):\n",
    "    if(result_class[i]==1):\n",
    "        ut += 1\n",
    "    else:\n",
    "        if(ut > 100):\n",
    "            signal = [abs ((scaled_data)[i]) for i in range(i-ut, i-1)]\n",
    "            energy = Energy(signal)\n",
    "            if(energy > 10):\n",
    "                events.append((\"ut\", i-ut, i-1, ut/freq))\n",
    "                print(num_events, \"- ut event,\" ,\"duration:\", ut/freq, \" tsteps\", \"init:\", i-ut ,\"end:\", i-1)\n",
    "                num_events += 1\n",
    "            ut = 0\n",
    "# add the dumb event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeIntensity(signal):\n",
    "    max_values = np.mean(nlargest(3, signal))\n",
    "    if(max_values > 0.1 and max_values < 0.2):\n",
    "        return 0\n",
    "    elif(max_values > 0.2 and max_values< 0.4):\n",
    "        return 1\n",
    "    elif(max_values > 0.4):\n",
    "        return 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assign intensity to each event \n",
    "0, 1, 2 --> Low, Medium, High\n",
    "thresholds:\n",
    "    0.98 0.1G\n",
    "    1.96 0.2G\n",
    "    3.92 0.4G\n",
    "\"\"\"    \n",
    "i = 0\n",
    "for e in events:\n",
    "    event_type, init, end, duration = e\n",
    "    signal = [abs(X_test)[i][8] for i in range(init, end)]\n",
    "    events[i] += (ComputeIntensity(signal),)\n",
    "    i +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "merging neighbour events\n",
    "\"\"\"\n",
    "duration = 0\n",
    "merge_index = 0\n",
    "deletion_index = []\n",
    "events.append(('',math.inf,math.inf,math.inf))\n",
    "\n",
    "for i in range(0,len(events)-1):\n",
    "    if(abs((events[i][2])-(events[i+1][1]))<11 and events[i][0]==events[i+1][0]):\n",
    "        duration +=1\n",
    "    else:\n",
    "        init = events[i-duration][1]\n",
    "        end = events[i][2]-1\n",
    "        events.append((events[i-duration][0], init, end, end-init, events[i-duration][4]))\n",
    "        deletion_index.append(np.arange(i-duration,i+1))\n",
    "        duration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.remove(('',math.inf,math.inf,math.inf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deletion_index\n",
    "del events[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_class = np.zeros(len(X_test))\n",
    "for e in events:\n",
    "    event_type, init, end, duration, intensity = e\n",
    "    if(event_type!=''):\n",
    "        if(event_type==\"ut\"):\n",
    "            result_class[init:end] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "y_true = y_test.copy()\n",
    "y_pred = result_class.copy()\n",
    "classes = [0,1]\n",
    "confusion_matrix = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "confusion_matrix\n",
    "# fig = px.imshow(confusion_matrix)\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig =tools.make_subplots(rows=1,cols=1)\n",
    "\n",
    "result =  go.Scatter(\n",
    "    x = list(range(0,7600)),\n",
    "    y = result_class,\n",
    "    mode = 'lines',\n",
    "    name='model_result'\n",
    ")\n",
    "\n",
    "test =  go.Scatter(\n",
    "    x = list(range(0,7600)),\n",
    "    y = y_test,\n",
    "    mode = 'lines',\n",
    "    name='test'\n",
    ")\n",
    "\n",
    "fig.append_trace(result,1,1)\n",
    "fig.append_trace(test,1,1)\n",
    "\n",
    "\n",
    "fig['layout'].update(height=500, title = 'Turning Model Classification')\n",
    "\n",
    "\n",
    "py.offline.plot(fig, filename='fold5_cross_validation_uturn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Precision = True predicted turnings / total turnings\n",
    "Recall = True predicted turnings / total predicted turnings\n",
    "\n",
    "F1 = ( 2 . precision . recall ) / precision + recall\n",
    "performance = F1 * (IOU / #turning)\n",
    "\"\"\"\n",
    "\n",
    "fold1_precision = 3 / 3\n",
    "fold1_recall = 4 / 6 \n",
    "fold1_f1 = (2 * fold1_precision * fold1_recall) / (fold1_precision + fold1_recall)\n",
    "fold1_iou = (112.0/807 +  719.0/3243 + 138.0/1129 + 109.0/1129) / 3\n",
    "fold1_perf = fold1_f1 * fold1_iou\n",
    "########\n",
    "fold2_precision = (2 / 3)\n",
    "fold2_recall = (2 / 2 )\n",
    "fold2_f1 = (2 * fold2_precision * fold2_recall) / (fold2_precision + fold2_recall)\n",
    "fold2_iou = (718.0/928 + 553.0/730) / 3\n",
    "fold2_perf = fold2_f1 * fold2_iou\n",
    "########\n",
    "fold3_precision = (3 / 3)\n",
    "fold3_recall = (3 / 3 )\n",
    "fold3_f1 = (2 * fold3_precision * fold3_recall) / (fold3_precision + fold3_recall)\n",
    "fold3_iou = (647.0/799 + 620.0/755 + 830.0/866) /3\n",
    "fold3_perf = fold3_f1 * fold3_iou\n",
    "########\n",
    "fold4_precision = (2 / 3)\n",
    "fold4_recall = (2 / 4 )\n",
    "fold4_f1 = (2 * fold4_precision * fold4_recall) / (fold4_precision + fold4_recall)\n",
    "fold4_iou = (605.0/ 705 + 264.0/1138) /2 \n",
    "fold4_perf = fold4_f1 * fold4_iou\n",
    "########\n",
    "fold5_precision = (2 / 3)\n",
    "fold5_recall = (2 / 3 )\n",
    "fold5_f1 = (2 * fold5_precision * fold5_recall) / (fold5_precision + fold5_recall)\n",
    "fold5_iou = (403.0/ 1018)  \n",
    "fold5_perf = fold5_f1 * fold5_iou\n",
    "########\n",
    "\n",
    "print(fold1_perf,fold2_perf,fold3_perf,fold4_perf,fold5_perf)\n",
    "print(\"performance mean:\", np.mean([fold1_perf, fold2_perf, fold3_perf, fold4_perf, fold5_perf]))\n",
    "print(\"f1-score mean:\", np.mean([fold1_f1,fold2_f1,fold3_f1,fold4_f1,fold5_f1]))\n",
    "print(\"iou mean:\", np.mean([fold1_iou,fold2_iou, fold3_iou,fold4_iou,fold5_iou]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "import os\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"./Learning_model/Turning/ut_cv/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"./Learning_model/Turning/ut_cv/model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "import os\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('./Learning_model/Turning/ut_cv/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"./Learning_model/Turning/ut_cv/model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "model = loaded_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env] *",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
